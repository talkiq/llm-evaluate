model_type: hf-automodel
model: t5-small
tokenizer_args:
  model_max_length: 3000
  truncation_side: right
  truncation: longest_first
model_load_args:
  task_type: T5ForConditionalGeneration
  device_map: balanced_low_0
  max_input_tokens: 3000
  torch_dtype: auto
model_inference_args:
  max_new_tokens: 512
  num_beams: 2
  do_sample: False
add_to_prompt_start: '[Prompt]'
add_to_prompt_end: '[Response]'
